# Chapter 7: Sizing and TTA
To perform state of the art training, the CrossEntropyFlat() loss function can be used instead as it flattens the input and target.
Normalisation is also identified as something that makes training models easier - but **note**, when using pretrained models, ensure the 
normalisation stats are the same that the model was trained on. This can be performed using the following line in the DataBlock:
```python
batch_tfms=[*aug_transforms(size=size, min_scale=0.75),
                               Normalize.from_stats(*imagenet_stats)])
```
Other aspects discussed in this chapter include:
- Progressing Resizing: Starting with smaller images and ending with larger images, to complete early training faster
- Test Time Augmentation: Using mutliple augmentations of the same image during validation, then taking the average or max. predictions for each augmented image, to improve accuracy without comprising on time.
- Mixup: Provides significantly higher accuracy in low data conditions. Involves
  1. Select a random image from the dataset
  2. Select a random weight
  3. Take the weighted average of the selected random image with your own, as the independent variable
  4. Take the weighted average of the selected random image's with your own image's labels, as the dependent variable
- Label Smoothing: Replaces the 0s with a number slightly higher, and 1s with a number slightly lower, to reduce confidence in outputs.

