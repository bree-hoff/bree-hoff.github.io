# A Brief Summary of Chapters 8 - 12
*All four chapters were completed in quick succession, so only a brief overview of my notes will be provided here*
## Brief Summary Table
I have collated some notes I made whilst progressing through these five chapters of the course. The information, I figured, might be useful to refer back to in later chapters, or might benefit some of my blog readers.
| Chapter 8 | Chapter 9 | Chapter 10 | Chapter 11 |  Chapter 12 |

|-|-|-|-|-|
| • Collaborative Filtering: Match users who have used of liked similar items, to recommend other items. | • Tabular modelling: Takes data in the form of a table.  | • NLP: Natural Language Processing, using self-supervised learning and often uses a pre-trained language model.  | • Mid-level API: Contains functionality for creating DataLoaders, and a callback system for customizing the traning loop and a general optimizer. | • Recurrent Neural Network: a Looping Network, using a for-loop in the LMModel1. |
| • Latent Factors: Use gradient descent to learn latent factors. | • Decision trees and random forests: Machine learning algorithms for classification tasks. | • Text Preprocessing: completed via tokenisation, numericalisation, creating a language model data loader and language model. | • Transforms: An object that behaves like a function to optionally initialise an inner state or reverse the function | • Hidden State: Updated activations at each step of a recurrent neural network.  |
| • Embedding and Biases: items with similar embedding values should have similar qualities, and biases indicate whether, for example, well or poorly matched users will rate a movie well.  | • Ensembling: averaging the predictions of models trained using different algorithms. | • Training text classifier: pre-trained models trained on a corpus like Wikipedia. | • TfmdLists and Datasets: TfmdLists group your pipeline with your raw item | • LSTMs: Maintain two hidden states, to have the correct info for the prediction of the next token, and retaining memory of everything that happened in the sentence. They are better than normal RNNs at memory retention |


## Points of Interest in Chapters 8-12
The following is a list of interesting information provided by the fastai course during my progression:
1. *Weight decay/ L2 regularisation in Chapter 8*:  adding the sum of all the weights squared to the loss function. This reduces overfitting and improves generalisation.
   ```python
    #hide_input
    #id parabolas
    x = np.linspace(-2,2,100)
    a_s = [1,2,5,10,50] 
    ys = [a * x**2 for a in a_s]
    _,ax = plt.subplots(figsize=(8,6))
    for a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}')
    ax.set_ylim([0,5])
    ax.legend();
   ```
   ![image](https://github.com/bree-hoff/bree-hoff.github.io/assets/111101248/46b7031f-3b6d-40da-9093-1a34822b7dbc) 
2. *Beyond Deep Learning in Chapter 9*: Ensembles of decision trees are often better suited to tabular data scenarios, as they train faster, are easier to interpret, and do not require specialised GPU hardware.
3. *Disinformation and Language Models in Chapter 10*: Simple language models can be used to bulk-create believable politically-driven comments/posts online, to influence policymakers, like that discovered by Jeff Kao, or even the general population.
4. *Dropped Neural Nets in Chapter 12*: The regularisation technique of dropped neural nets helps neuron cooperation, and increases the noisiness of activations, improving the model's robustness
    ![image](https://github.com/bree-hoff/bree-hoff.github.io/assets/111101248/1be5e348-b010-4dc8-b73e-103d796feb15) \

Thanks for reading!
