# A Brief Summary of Chapters 8 - 12
*All four chapters were completed in quick succession, so only a brief overview of my notes will be provided here*
## Brief Summary Table

| Chapter 8 | Chapter 9 | Chapter 10 | Chapter 11 |  Chapter 12 |
|-|-|-|-|-|
| • Collaborative Filtering: Match users who have used of liked similar items, to recommend other items. | • Tabular modelling: Takes data in the form of a table  | • NLP: Natural Language Processing, using self-supervised learning and often uses a pre-trained language model  | • Item 13 | • Item 13 |
| • Latent Factors: Use gradient descent to learn latent factors | • Decision trees and random forests: Machine learning algorithms for classification tasks | • Text Preprocessing: completed via tokenisation, numericalisation, creating a language model data loader and loaguage model | • Item 14 | • Item 13 |
| • Embedding and Biases: items with similar embedding values should have similar qualities, and biases indicate whether, for example, well or poorly matched users will rate a movie well  | • Ensembling: averaging the predictions of models trained using different algorithms | • Training text classifier: pre-trained models trained on a corpus like Wikipedia | • Item 15 | • Item 13 |

## Points of Interest in Chapters 8-12
1. *Weight decay/ L2 regularisation in Chapter 8*:  adding the sum of all the weights squared to the loss function. This reduces overfitting and improves generalisation.
   ```python
    #hide_input
    #id parabolas
    x = np.linspace(-2,2,100)
    a_s = [1,2,5,10,50] 
    ys = [a * x**2 for a in a_s]
    _,ax = plt.subplots(figsize=(8,6))
    for a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}')
    ax.set_ylim([0,5])
    ax.legend();
   ```
   ![image](https://github.com/bree-hoff/bree-hoff.github.io/assets/111101248/46b7031f-3b6d-40da-9093-1a34822b7dbc) 
2. *Beyond Deep Learning in Chapter 9*: Ensembles of decision trees are often better suited to tabular data scenarios, as they train faster, are easier to interpret, and do not require specialised GPU hardware.
3. *Disinformation and Language Models in Chapter 10*: Simple language models can be used to bulk-create believable politically-driven comments/posts online, to influence policymakers, like that discovered by Jeff Kao, or even the general population.
4. 

## Basic setup

Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-filename.md`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `filename` is whatever file name you choose, to remind yourself what this post is about. `.md` is the file extension for markdown files.

The first line of the file should start with a single hash character, then a space, then your title. This is how you create a "*level 1 heading*" in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line `## File names` above.

## Basic formatting

You can use *italics*, **bold**, `code font text`, and create [links](https://www.markdownguide.org/cheat-sheet/). Here's a footnote [^1]. Here's a horizontal rule:

---

## Lists

Here's a list:

- item 1
- item 2

And a numbered list:

1. item 1
1. item 2

## Boxes and stuff

> This is a quotation

{% include alert.html text="You can include alert boxes" %}

...and...

{% include info.html text="You can include info boxes" %}

## Images

![](/images/logo.png "fast.ai's logo")

## Code

General preformatted text:

    # Do a thing
    do_thing()

Python code and output:

```python
# Prints '2'
print(1+1)
```

    2

## Tables

| Column 1 | Column 2 |
|-|-|
| A thing | Another thing |

## Footnotes

[^1]: This is the footnote.


