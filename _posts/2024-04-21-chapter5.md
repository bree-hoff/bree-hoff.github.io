# Chapter 5: Pet Breeds
Chapter 5 delved into picking discriminative learning rates, pre-sizing, cross-entropy loss, and the use of a custom DataBlock for image
classification.
To filter filenames pertaining to specific breeds, a regular expression can be used: refer to a [Regular Expressions Cheat Sheet](https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf).
The chapter highlights pre-sizing as something unseens in previous chapters - cropping and augmentation are necessary for improved 
generalization, robustness and consistent input formats for the model:
```python
item_tfms=Resize(460),
batch_tfms=aug_transforms(size=224, min_scale=0.75)
```
### Pre-sizing and augmentation steps
1. The images should be resized to notably larger dimensions than those required for target training dimensions.
2. These images are then augmented, cropped and resized to a square.
3. All augmentation operations are then performed together

## Cross-Entropy Loss
Cross-entropy as a loss function that produces fast and reliable deep-learning training by taking the softmaxe followed by the log likelihood, and is appropriate for
multi-class classification. It was interesting to learn that the softmax function is simply the multi-category equivalent of the sigmoid function.
The gradient of cross-entropy is proportional to the difference between the prediction and the target.
The logarithmic aspect of cross-entropy allows multiplication to be replaced with additions, ensuring better outputs for very large and very small
values. 
```python
plot_function(lambda x: -1*torch.log(x), min=0,max=1, tx='x', ty='- log(x)', title = 'Log Loss when true label = 1')
```
![image](https://github.com/bree-hoff/bree-hoff.github.io/assets/111101248/beb7e0ad-1bd2-410b-bf58-9b94e44222a6) \

## Model Interpretation and Improvement
The model interpretation produced a huge confusion matrix, due to 37 breeds being learned. **Pro-tip**: use the most_confused() function
to produce a confusion matrix with only the most confused cells. 
![image](https://github.com/bree-hoff/bree-hoff.github.io/assets/111101248/9dd1c9e3-54f7-4298-b416-657862e5fa62)

Many epochs are required for a small learning rate, leading to high training time and ofitting of data. Conversly, the model won't improve
at all if the learning rate is too large. Leslie Smith suggest the following:
1. Begin with a particularly small learning rate
2. Use it on one min-batch
3. Derive the losses
4. Increase the learning rate by a fixed percentage
5. Repeat 1-4 until the loss degrades
