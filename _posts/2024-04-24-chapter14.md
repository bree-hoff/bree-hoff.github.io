# A Brief Summary of Chapters 8 - 12
*All four chapters were completed in quick succession, so only a brief overview of my notes will be provided here*
## Brief Summary Table

| Chapter 8 | Chapter 9 | Chapter 10 | Chapter 11 |  Chapter 12 |
|-|-|-|-|-|
| • Collaborative Filtering: Match users who have used of liked similar items, to recommend other items. | • Tabular modelling: Takes data in the form of a table.  | • NLP: Natural Language Processing, using self-supervised learning and often uses a pre-trained language model.  | • Mid-level API: Contains functionality for creating DataLoaders, and a callback system for customizing the traning loop and a general optimizer. | • Recurrent Neural Network: a Looping Network, using a for-loop in the LMModel1. |
| • Latent Factors: Use gradient descent to learn latent factors. | • Decision trees and random forests: Machine learning algorithms for classification tasks. | • Text Preprocessing: completed via tokenisation, numericalisation, creating a language model data loader and language model. | • Transforms: An object that behaves like a function to optionally initialise an inner state or reverse the function | • Hidden State: Updated activations at each step of a recurrent neural network.  |
| • Embedding and Biases: items with similar embedding values should have similar qualities, and biases indicate whether, for example, well or poorly matched users will rate a movie well.  | • Ensembling: averaging the predictions of models trained using different algorithms. | • Training text classifier: pre-trained models trained on a corpus like Wikipedia. | • TfmdLists and Datasets: TfmdLists group your pipeline with your raw item | • LSTMs: Maintain two hidden states, to have the correct info for the prediction of the next token, and retaining memory of everything that happened in the sentence. They are better than normal RNNs at memory retention |

## Points of Interest in Chapters 8-12
1. *Weight decay/ L2 regularisation in Chapter 8*:  adding the sum of all the weights squared to the loss function. This reduces overfitting and improves generalisation.
   ```python
    #hide_input
    #id parabolas
    x = np.linspace(-2,2,100)
    a_s = [1,2,5,10,50] 
    ys = [a * x**2 for a in a_s]
    _,ax = plt.subplots(figsize=(8,6))
    for a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}')
    ax.set_ylim([0,5])
    ax.legend();
   ```
   ![image](https://github.com/bree-hoff/bree-hoff.github.io/assets/111101248/46b7031f-3b6d-40da-9093-1a34822b7dbc) 
2. *Beyond Deep Learning in Chapter 9*: Ensembles of decision trees are often better suited to tabular data scenarios, as they train faster, are easier to interpret, and do not require specialised GPU hardware.
3. *Disinformation and Language Models in Chapter 10*: Simple language models can be used to bulk-create believable politically-driven comments/posts online, to influence policymakers, like that discovered by Jeff Kao, or even the general population.
4. *Dropped Neural Nets in Chapter 12*: The regularisation technique of dropped neural nets helps neuron cooperation, and increases the noisiness of activations, improving the model's robustness
    ![image](https://github.com/bree-hoff/bree-hoff.github.io/assets/111101248/1be5e348-b010-4dc8-b73e-103d796feb15) \

## Basic setup

Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-filename.md`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `filename` is whatever file name you choose, to remind yourself what this post is about. `.md` is the file extension for markdown files.

The first line of the file should start with a single hash character, then a space, then your title. This is how you create a "*level 1 heading*" in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line `## File names` above.

## Basic formatting

You can use *italics*, **bold**, `code font text`, and create [links](https://www.markdownguide.org/cheat-sheet/). Here's a footnote [^1]. Here's a horizontal rule:

---

## Lists

Here's a list:

- item 1
- item 2

And a numbered list:

1. item 1
1. item 2

## Boxes and stuff

> This is a quotation

{% include alert.html text="You can include alert boxes" %}

...and...

{% include info.html text="You can include info boxes" %}

## Images

![](/images/logo.png "fast.ai's logo")

## Code

General preformatted text:

    # Do a thing
    do_thing()

Python code and output:

```python
# Prints '2'
print(1+1)
```

    2

## Tables

| Column 1 | Column 2 |
|-|-|
| A thing | Another thing |

## Footnotes

[^1]: This is the footnote.


